
@misc{negi2025a,
	title = {Optimizing {Language} {Model} {Embeddings} to {Voxel} {Activity} {Improves} {Brain} {Activity} {Predictions}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.09.18.676935v1},
	doi = {10.1101/2025.09.18.676935},
	abstract = {Recent studies have shown that contextual semantic embeddings from language models can accurately predict human brain activity during language processing. However, most studies use contextual embeddings with the same context length and model layer for all voxels, potentially overlooking meaningful variations across the brain. In this study, we investigate whether optimizing contextual embeddings for individual voxels improves their ability to predict brain activity during reading. We optimize embeddings for each voxel by selecting the best-predicting context length, model layer, or both. We perform this optimization with two different types of stimuli (isolated sentences and narratives), and quantify the performance gains of optimized embeddings over standard fixed embeddings. Our results show that voxel-specific optimization substantially improves the prediction accuracy of contextual semantic embeddings. These findings demonstrate that voxel-specific contextual tuning provides a more accurate and nuanced account of how the contextual semantic information is represented across the cortex.},
	language = {en},
	urldate = {2025-09-22},
	publisher = {bioRxiv},
	author = {Anuja Negi and Christine Tseng and Anwar O. Nunez-Elizalde and Lily Xue Gong and Fatma Deniz},
	month = sep,
	year = {2025},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/5SJ98ZPM/Negi et al. - 2025 - Optimizing Language Model Embeddings to Voxel Activity Improves Brain Activity Predictions.pdf:application/pdf},
	preview = {negi2025optimizing.jpg},
	selected = {true}
}

@misc{viscontidioleggiocastello2025,
	title = {Encoding models in functional magnetic resonance imaging: the {Voxelwise} {Encoding} {Model} framework},
	shorttitle = {Encoding models in functional magnetic resonance imaging},
	url = {https://sciety-labs.elifesciences.org/articles/by?article_doi=10.31234/osf.io/nt2jq_v2},
	abstract = {One of the major goals of cognitive neuroscience is understanding how the brain represents information about its own internal states and about the external world. This goal can be addressed by creating encoding models that reveal the information represented explicitly in measured brain activity. Here we describe the Voxelwise Encoding Model (VEM) framework for creating encoding models with functional magnetic resonance imaging (fMRI) data. The VEM framework provides several key advantages over traditional neuroimaging approaches. First, the VEM framework is applicable to most experimental designs, from classic factorial designs to complex naturalistic experiments such as movie watching or video games. This flexibility enables researchers to study brain function across multiple domains with the same analytical approach. Second, hypotheses about functional representations are defined and tested quantitatively by extracting feature spaces from experimental stimuli or tasks. These feature spaces quantify specific types of information potentially represented in brain activity and can range from simple human-derived labels to complex features generated by deep neural networks. If a feature space can be used to linearly predict brain activity, the brain is considered to explicitly represent features within that feature space. This prediction approach provides a systematic way to test hypotheses about functional representations. Third, the VEM framework implements robust data science methods to improve model estimation and minimize false positive results. Encoding models are estimated on a training set and validated on an independent test set. Testing in an independent dataset provides direct evidence that experimental findings generalize beyond the dataset used for model estimation. Finally, voxelwise encoding models can be created in each participant's native brain space without unnecessary information loss due to spatial averaging or template resampling required by conventional group analyses. This enables the VEM approach to reveal fine-grained functional organization in individual participants that might otherwise be ignored. In this article, we provide a comprehensive guide to the Voxelwise Encoding Model framework through all phases of research, from experimental design and data collection to the estimation and interpretation of voxelwise encoding models.},
	language = {en},
	urldate = {2025-09-22},
	journal = {ScietyLabs},
	author = {Matteo Visconti Di Oleggio Castello and Fatma Deniz and Tom Dupré La Tour and Jack L. Gallant},
	month = sep,
	year = {2025},
	file = {Snapshot:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/IKA9A2GT/by.html:text/html},
	selected = {true}
}

@article{negi2025,
	title = {Brain-{Informed} {Fine}-{Tuning} for {Improved} {Multilingual} {Understanding} in {Language} {Models}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.07.07.662360v1},
	doi = {10.1101/2025.07.07.662360},
	abstract = {Recent studies have demonstrated that fine-tuning language models with brain data can improve their semantic understanding, although these findings have so far been limited to English. Interestingly, similar to the shared multilingual embedding space of pretrained multilingual language models, human studies provide strong evidence for a shared semantic system in bilingual individuals. Here, we investigate whether fine-tuning language models with bilingual brain data changes model representations in a way that improves them across multiple languages. To test this, we fine-tune monolingual and multilingual language models using brain activity recorded while bilingual participants read stories in English and Chinese. We then evaluate how well these representations generalize to the bilingual participants’ first language, their second language, and several other languages that the participant is not fluent in. We assess the fine-tuned language models on brain encoding performance and downstream NLP tasks. Our results show that bilingual brain-informed fine-tuned language models outperform their vanilla (pretrained) counterparts in both brain encoding performance and most downstream NLP tasks across multiple languages. These findings suggest that brain-informed fine-tuning improves multilingual understanding in language models, offering a bridge between cognitive neuroscience and NLP research.},
	language = {en},
	urldate = {2025-09-22},
	publisher = {NeurIPS},
	author = {Anuja Negi and Subba Reddy Oota and Anwar O. Nunez-Elizalde and Manish Gupta and Fatma Deniz},
	month = jul,
	year = {2025},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/GL82YADH/Negi et al. - 2025 - Brain-Informed Fine-Tuning for Improved Multilingual Understanding in Language Models.pdf:application/pdf},
	preview = {negi2025brain.png}
}

@article{chen2024,
	title = {The cortical representation of language timescales is shared between reading and listening},
	volume = {7},
	copyright = {2024 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-024-05909-z},
	doi = {10.1038/s42003-024-05909-z},
	abstract = {Language comprehension involves integrating low-level sensory inputs into a hierarchy of increasingly high-level features. Prior work studied brain representations of different levels of the language hierarchy, but has not determined whether these brain representations are shared between written and spoken language. To address this issue, we analyze fMRI BOLD data that were recorded while participants read and listened to the same narratives in each modality. Levels of the language hierarchy are operationalized as timescales, where each timescale refers to a set of spectral components of a language stimulus. Voxelwise encoding models are used to determine where different timescales are represented across the cerebral cortex, for each modality separately. These models reveal that between the two modalities timescale representations are organized similarly across the cortical surface. Our results suggest that, after low-level sensory processing, language integration proceeds similarly regardless of stimulus modality.},
	language = {en},
	number = {1},
	urldate = {2025-09-22},
	journal = {Communications Biology},
	author = {Catherine Chen and Tom Dupré La Tour and Jack L. Gallant and Daniel Klein and Fatma Deniz},
	month = mar,
	year = {2024},
	keywords = {Computational neuroscience, Language},
	pages = {284},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/5RFEBLPW/Chen et al. - 2024 - The cortical representation of language timescales is shared between reading and listening.pdf:application/pdf},
	preview = {ReadingListening1.png},
	selected = {true}
}

@book{kitzes2017,
	title = {The {Practice} of {Reproducible} {Research} {Case} {Studies} and {Lessons} from the {Data}-{Intensive} {Sciences}},
	url = {https://www.ucpress.edu/books/the-practice-of-reproducible-research/paper},
	abstract = {The Practice of Reproducible Research presents concrete examples of how researchers in the data-intensive sciences are working to improve the reproducibility of their research projects. In each of the thirty-one case studies in this volume, the author or team describes the workflow that they used to complete a real-world research project. Authors highlight how they utilized particular tools, ideas, and practices to support reproducibility, emphasizing the very practical how, rather than the why or what, of conducting reproducible research.},
	language = {en},
	urldate = {2025-09-22},
	publisher = {University Of California Press},
	editor = {Justin Kitzes and Daniel Turek and Fatma Deniz},
	year = {2017},
	file = {Snapshot:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/BYTFYHV2/paper.html:text/html},
	preview = {The Practice of Reproducible Research-thumbnail.jpg}
}

@article{elfaramawy2024,
	title = {On managing large collections of scientific workflows},
	publisher = {Gesellschaft für Informatik e.V.},
	author = {Nourhan Elfaramawy and Fatma Deniz and Lars Grunske and Marcus Hilbrich and Timo Kehrer and Anna-Lena Lamprecht and Jan Mendling and Matthias Weidlich},
	year = {2024},
	doi = {10.18420/modellierung2024-ws-012},
	preview = {elfaramawy et al 2024-thumbnail.png}
}

@article{gong2023,
	title = {Phonemic segmentation of narrative speech in human cerebral cortex},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-39872-w},
	doi = {10.1038/s41467-023-39872-w},
	abstract = {Speech processing requires extracting meaning from acoustic patterns using a set of intermediate representations based on a dynamic segmentation of the speech stream. Using whole brain mapping obtained in fMRI, we investigate the locus of cortical phonemic processing not only for single phonemes but also for short combinations made of diphones and triphones. We find that phonemic processing areas are much larger than previously described: they include not only the classical areas in the dorsal superior temporal gyrus but also a larger region in the lateral temporal cortex where diphone features are best represented. These identified phonemic regions overlap with the lexical retrieval region, but we show that short word retrieval is not sufficient to explain the observed responses to diphones. Behavioral studies have shown that phonemic processing and lexical retrieval are intertwined. Here, we also have identified candidate regions within the speech cortical network where this joint processing occurs.},
	language = {en},
	number = {1},
	urldate = {2025-09-22},
	journal = {Nature Communications},
	author = {Xue L. Gong and Alexander G. Huth and Fatma Deniz and Keith Johnson and Jack L. Gallant and Frédéric E. Theunissen},
	month = jul,
	year = {2023},
	keywords = {Cortex, Language, Neural encoding},
	pages = {4309},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/28FQ2WA7/Gong et al. - 2023 - Phonemic segmentation of narrative speech in human cerebral cortex.pdf:application/pdf},
	preview = {gong et al 2023-thumbnail.png}
}

@incollection{deniz2017,
	title = {{pyMooney}: {Generating} a {Database} of {Two}-{Tone}, {Mooney} {Images}},
	url = {https://www.practicereproducibleresearch.org/},
	booktitle = {The {Practice} of {Reproducible} {Research}: {Case} {Studies} in {Data} {Science}},
	author = {Fatma Deniz},
	year = {2017},
	preview = {mooney_animation-thumbnail.gif}
}

@incollection{deniz2017a,
	title = {{Introducing} the {Case} {Studies}.},
	url = {https://www.practicereproducibleresearch.org/},
	booktitle = {The {Practice} of {Reproducible} {Research}: {Case} {Studies} in {Data} {Science}},
	author = {Daniel Turek and Fatma Deniz},
	year = {2017},
	preview = {mooney_animation-thumbnail.gif}
}

@inproceedings{lamarre-etal-2022-attention,
    title = "Attention weights accurately predict language representations in the brain",
    author = {Mathis Lamarre  and Catherine Chen and Fatma Deniz},
    editor = {Yoav Goldberg and Zornitsa Kozareva and Yue Zhang},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = {2022},
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.330/",
    doi = "10.18653/v1/2022.findings-emnlp.330",
    pages = "4513--4529",
    abstract = "In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the attention mechanism constructs contextual embeddings that carry information reflected in language-related brain representations. However, it is unclear whether the attention weights that are used to integrate information across words are themselves related to language representations in the brain. To address this question we analyzed functional magnetic resonance imaging (fMRI) recordings of participants reading English language narratives. We provided the narrative text as input to two LMs (BERT and GPT-2) and extracted their corresponding attention weights. We then used encoding models to determine how well attention weights can predict recorded brain responses. We find that attention weights accurately predict brain responses in much of the frontal and temporal cortices. Our results suggest that the attention mechanism itself carries information that is reflected in brain representations. Moreover, these results indicate cortical areas in which context integration may occur.",
    preview = {lamarre2022.png}
}


@inproceedings{oota2024,
	address = {Bangkok, Thailand},
	title = {Speech language models lack important brain-relevant semantics},
	url = {https://aclanthology.org/2024.acl-long.462/},
	doi = {10.18653/v1/2024.acl-long.462},
	abstract = {Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available. [https://github.com/subbareddy248/speech-llm-brain]},
	urldate = {2025-10-02},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Subba Reddy Oota and Emin Çelik and Fatma Deniz and Mariya Toneva},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {8503--8528},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/TCKQB9Q2/Oota et al. - 2024 - Speech language models lack important brain-relevant semantics.pdf:application/pdf},
}

@article{deniz2023,
	title = {Semantic {Representations} during {Language} {Comprehension} {Are} {Affected} by {Context}},
	volume = {43},
	copyright = {Copyright © 2023 the authors. SfN exclusive license.},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/43/17/3144},
	doi = {10.1523/JNEUROSCI.2459-21.2023},
	abstract = {The meaning of words in natural language depends crucially on context. However, most neuroimaging studies of word meaning use isolated words and isolated sentences with little context. Because the brain may process natural language differently from how it processes simplified stimuli, there is a pressing need to determine whether prior results on word meaning generalize to natural language. fMRI was used to record human brain activity while four subjects (two female) read words in four conditions that vary in context: narratives, isolated sentences, blocks of semantically similar words, and isolated words. We then compared the signal-to-noise ratio (SNR) of evoked brain responses, and we used a voxelwise encoding modeling approach to compare the representation of semantic information across the four conditions. We find four consistent effects of varying context. First, stimuli with more context evoke brain responses with higher SNR across bilateral visual, temporal, parietal, and prefrontal cortices compared with stimuli with little context. Second, increasing context increases the representation of semantic information across bilateral temporal, parietal, and prefrontal cortices at the group level. In individual subjects, only natural language stimuli consistently evoke widespread representation of semantic information. Third, context affects voxel semantic tuning. Finally, models estimated using stimuli with little context do not generalize well to natural language. These results show that context has large effects on the quality of neuroimaging data and on the representation of meaning in the brain. Thus, neuroimaging studies that use stimuli with little context may not generalize well to the natural regime.
SIGNIFICANCE STATEMENT Context is an important part of understanding the meaning of natural language, but most neuroimaging studies of meaning use isolated words and isolated sentences with little context. Here, we examined whether the results of neuroimaging studies that use out-of-context stimuli generalize to natural language. We find that increasing context improves the quality of neuro-imaging data and changes where and how semantic information is represented in the brain. These results suggest that findings from studies using out-of-context stimuli may not generalize to natural language used in daily life.},
	language = {en},
	number = {17},
	urldate = {2025-10-02},
	journal = {Journal of Neuroscience},
	author = {Fatma Deniz and Christine Tseng and Leila Wehbe and Tom Dupré La Tour and Jack L. Gallant},
	month = apr,
	year = {2023},
	pmid = {36973013},
	pages = {3144--3158},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/UUQTU6J6/Deniz et al. - 2023 - Semantic Representations during Language Comprehension Are Affected by Context.pdf:application/pdf},
}

@article{popham2021,
	title = {Visual and linguistic semantic representations are aligned at the border of human visual cortex},
	volume = {24},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00921-6},
	doi = {10.1038/s41593-021-00921-6},
	abstract = {Semantic information in the human brain is organized into multiple networks, but the fine-grain relationships between them are poorly understood. In this study, we compared semantic maps obtained from two functional magnetic resonance imaging experiments in the same participants: one that used silent movies as stimuli and another that used narrative stories. Movies evoked activity from a network of modality-specific, semantically selective areas in visual cortex. Stories evoked activity from another network of semantically selective areas immediately anterior to visual cortex. Remarkably, the pattern of semantic selectivity in these two distinct networks corresponded along the boundary of visual cortex: for visual categories represented posterior to the boundary, the same categories were represented linguistically on the anterior side. These results suggest that these two networks are smoothly joined to form one contiguous map.},
	language = {en},
	number = {11},
	urldate = {2025-10-02},
	journal = {Nature Neuroscience},
	author = {Sara F. Popham and Alexander G. Huth and Natalia Y. Bilenko and Fatma Deniz and James S. Gao and Anwar O. Nunez-Elizalde and Jack L. Gallant},
	month = nov,
	year = {2021},
	keywords = {Language, Neural encoding, Perception, Visual system},
	pages = {1628--1636},
}

@inproceedings{ramakrishnan2021,
	address = {Online},
	title = {Non-{Complementarity} of {Information} in {Word}-{Embedding} and {Brain} {Representations} in {Distinguishing} between {Concrete} and {Abstract} {Words}},
	url = {https://aclanthology.org/2021.cmcl-1.1/},
	doi = {10.18653/v1/2021.cmcl-1.1},
	abstract = {Word concreteness and imageability have proven crucial in understanding how humans process and represent language in the brain. While word-embeddings do not explicitly incorporate the concreteness of words into their computations, they have been shown to accurately predict human judgments of concreteness and imageability. Inspired by the recent interest in using neural activity patterns to analyze distributed meaning representations, we first show that brain responses acquired while human subjects passively comprehend natural stories can significantly distinguish the concreteness levels of the words encountered. We then examine for the same task whether the additional perceptual information in the brain representations can complement the contextual information in the word-embeddings. However, the results of our predictive models and residual analyses indicate the contrary. We find that the relevant information in the brain representations is a subset of the relevant information in the contextualized word-embeddings, providing new insight into the existing state of natural language processing models.},
	urldate = {2025-10-02},
	booktitle = {Proceedings of the {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Kalyan Ramakrishnan and Fatma Deniz},
	editor = {Chersoni, Emmanuele and Hollenstein, Nora and Jacobs, Cassandra and Oseki, Yohei and Prévot, Laurent and Santus, Enrico},
	month = jun,
	year = {2021},
	pages = {1--11},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/235CUNTF/Ramakrishnan and Deniz - 2021 - Non-Complementarity of Information in Word-Embedding and Brain Representations in Distinguishing bet.pdf:application/pdf},
}

@article{deniz2019,
	title = {The {Representation} of {Semantic} {Information} {Across} {Human} {Cerebral} {Cortex} {During} {Listening} {Versus} {Reading} {Is} {Invariant} to {Stimulus} {Modality}},
	volume = {39},
	copyright = {Copyright © 2019 the authors},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/39/39/7722},
	doi = {10.1523/JNEUROSCI.0675-19.2019},
	abstract = {An integral part of human language is the capacity to extract meaning from spoken and written words, but the precise relationship between brain representations of information perceived by listening versus reading is unclear. Prior neuroimaging studies have shown that semantic information in spoken language is represented in multiple regions in the human cerebral cortex, while amodal semantic information appears to be represented in a few broad brain regions. However, previous studies were too insensitive to determine whether semantic representations were shared at a fine level of detail rather than merely at a coarse scale. We used fMRI to record brain activity in two separate experiments while participants listened to or read several hours of the same narrative stories, and then created voxelwise encoding models to characterize semantic selectivity in each voxel and in each individual participant. We find that semantic tuning during listening and reading are highly correlated in most semantically selective regions of cortex, and models estimated using one modality accurately predict voxel responses in the other modality. These results suggest that the representation of language semantics is independent of the sensory modality through which the semantic information is received.
SIGNIFICANCE STATEMENT Humans can comprehend the meaning of words from both spoken and written language. It is therefore important to understand the relationship between the brain representations of spoken or written text. Here, we show that although the representation of semantic information in the human brain is quite complex, the semantic representations evoked by listening versus reading are almost identical. These results suggest that the representation of language semantics is independent of the sensory modality through which the semantic information is received.},
	language = {en},
	number = {39},
	urldate = {2025-10-02},
	journal = {Journal of Neuroscience},
	author = {Fatma Deniz and Anwar O. Nunez-Elizalde and Alexander G. Huth and Jack L. Gallant},
	month = sep,
	year = {2019},
	pmid = {31427396},
	keywords = {BOLD, cross-modal representations, fMRI, listening, reading, semantics},
	pages = {7722--7736},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/GDBL883V/Deniz et al. - 2019 - The Representation of Semantic Information Across Human Cerebral Cortex During Listening Versus Read.pdf:application/pdf},
}

@inproceedings{castelluccia2017,
	title = {Towards {Implicit} {Visual} {Memory}-{Based} {Authentication}},
	url = {https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/towards-implicit-visual-memory-based-authentication/},
	abstract = {Selecting and remembering secure passwords puts a high cognitive burden on the user, which has adverse effects on usability and security. Authentication schemes based on implicit memory can relieve the user of the burden of actively remembering a secure password. In this paper, we propose a new authentication scheme (MooneyAuth) that relies on implicitly remembering the content of previously seen Mooney images. These images are thresholded two-tone images derived from images containing single objects. Our scheme has two phases: In the enrollment phase, a user is presented with Mooney images, their corresponding original images, and labels. This creates an implicit link between the Mooney image and the object in the user   s memory that serves as the authentication secret. In the authentication phase, the user has to label a set of Mooney images, a task that gets performed with substantially fewer mistakes if the images have been seen in the enrollment phase. We applied an information-theoretical approach to compute the eligibility of the user, based on which images were labeled correctly. This new dynamic scoring is substantially better than previously proposed static scoring by considering the surprisal of the observed events. We built a prototype and performed three experiments with 230 and 70 participants over the course of 264 and 21 days, respectively. We show that MooneyAuth outperforms current implicit memory-based schemes, and demonstrates a promising new approach for fallback authentication procedures on the Web.},
	author = {Claude Castelluccia and Markus Dürmuth and Maximilian Golla and Fatma Deniz},
	language = {en-US},
	urldate = {2025-10-02},
	year = {2017},
	booktitle = {NDSS Symposium},
	file = {Snapshot:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/63Q2RJRX/towards-implicit-visual-memory-based-authentication.html:text/html},
}

@inproceedings{holdgraf2017,
	address = {New York, NY, USA},
	series = {{PEARC} '17},
	title = {Portable {Learning} {Environments} for {Hands}-{On} {Computational} {Instruction}: {Using} {Container}- and {Cloud}-{Based} {Technology} to {Teach} {Data} {Science}},
	isbn = {978-1-4503-5272-7},
	shorttitle = {Portable {Learning} {Environments} for {Hands}-{On} {Computational} {Instruction}},
	url = {https://doi.org/10.1145/3093338.3093370},
	doi = {10.1145/3093338.3093370},
	abstract = {There is an increasing interest in learning outside of the traditional classroom setting. This is especially true for topics covering computational tools and data science, as both are challenging to incorporate in the standard curriculum. These atypical learning environments offer new opportunities for teaching, particularly when it comes to combining conceptual knowledge with hands-on experience/expertise with methods and skills. Advances in cloud computing and containerized environments provide an attractive opportunity to improve the efficiency and ease with which students can learn. This manuscript details recent advances towards using commonly-available cloud computing services and advanced cyberinfrastructure support for improving the learning experience in bootcamp-style events. We cover the benefits (and challenges) of using a server hosted remotely instead of relying on student laptops, discuss the technology that was used in order to make this possible, and give suggestions for how others could implement and improve upon this model for pedagogy and reproducibility.},
	urldate = {2025-10-02},
	booktitle = {Practice and {Experience} in {Advanced} {Research} {Computing} 2017: {Sustainability}, {Success} and {Impact}},
	publisher = {Association for Computing Machinery},
	author = {Chris Holdgraf and Aaron Culich and Ariel Rokem and Fatma Deniz and Maryana Alegro and Dani Ushizima},
	month = jul,
	year = {2017},
	pages = {1--9},
	file = {Full Text:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/RZ25LHVT/Holdgraf et al. - 2017 - Portable Learning Environments for Hands-On Computational Instruction Using Container- and Cloud-Ba.pdf:application/pdf},
}

@article{kizilirmak2016,
	title = {Generation and the subjective feeling of “aha!” are independently related to learning from insight},
	volume = {80},
	issn = {0340-0727},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC5069302/},
	doi = {10.1007/s00426-015-0697-2},
	abstract = {It has been proposed that sudden insight into the solutions of problems can enhance long-term memory for those solutions. However, the nature of insight has been operationalized differently across studies. Here, we examined two main aspects of insight problem-solving—the generation of a solution and the subjective “aha!” experience—and experimentally evaluated their respective relationships to long-term memory formation (encoding). Our results suggest that generation (generated solution vs. presented solution) and the “aha!” experience (“aha!” vs. no “aha!”) are independently related to learning from insight, as well as to the emotional response towards understanding the solution during encoding. Moreover, we analyzed the relationship between generation and the “aha!” experience and two different kinds of later memory tests, direct (intentional) and indirect (incidental). Here, we found that the generation effect was larger for indirect testing, reflecting more automatic retrieval processes, while the relationship with the occurrence of an “aha!” experience was somewhat larger for direct testing. Our results suggest that both the generation of a solution and the subjective experience of “aha!” indicate processes that benefit long-term memory formation, though differently. This beneficial effect is possibly due to the intrinsic reward associated with sudden comprehension and the detection of schema-consistency, i.e., that novel information can be easily integrated into existing knowledge.},
	number = {6},
	urldate = {2025-10-02},
	journal = {Psychological Research},
	author = {Jasmin M. Kizilirmak and Joana Galvao Gomes da Silva and Fatma Imamoglu and Alan Richardson-Klavehn},
	year = {2016},
	pmid = {26280758},
	pmcid = {PMC5069302},
	pages = {1059--1074},
	file = {Full Text:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/LNT2TINC/Kizilirmak et al. - 2016 - Generation and the subjective feeling of “aha!” are independently related to learning from insight.pdf:application/pdf},
}

@inproceedings{castelluccia2014,
	title = {Learning from {Neuroscience} to {Improve} {Internet} {Security}},
	url = {https://ercim-news.ercim.eu/en99/ri/learning-from-neuroscience-to-improve-internet-security},
	urldate = {2025-10-02},
	author = {Claude Castelluccia and Markus Duermuth and Fatma Imamoglu},
	year = {2014},
	booktitle = {ERCIM News},
	file = {Learning from Neuroscience to Improve Internet Security:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/SL2UV4N2/learning-from-neuroscience-to-improve-internet-security.html:text/html},
}

@article{imamoglu2014,
	title = {Activity in high-level brain regions reflects visibility of low-level stimuli},
	volume = {102 Pt 2},
	issn = {1095-9572},
	doi = {10.1016/j.neuroimage.2014.08.045},
	abstract = {Stimulus visibility is associated with neural signals in multiple brain regions, ranging from visual cortex to prefrontal regions. Here we used functional magnetic resonance imaging (fMRI) to investigate to which extent the perceived visibility of a "low-level" grating stimulus is reflected in the brain activity in high-level brain regions. Oriented grating stimuli were presented under varying visibility conditions created by backward masking. Visibility was manipulated using four different stimulus onset asynchronies (SOAs), which created a continuum from invisible to highly visible target stimuli. Brain activity in early visual areas, high-level visual brain regions (fusiform gyrus), as well as parietal and prefrontal brain regions was significantly correlated with subjects' psychometric visibility functions. In addition, increased stimulus visibility was reflected in the functional coupling between low and high-level visual areas. Specifically, neuroimaging signals in the middle occipital gyrus were significantly more correlated with signals in the inferior temporal gyrus when subjects successfully perceived the target stimulus than when they did not. These results provide evidence that not only low-level visual but also high-level brain regions reflect visibility of low-level grating stimuli and that changes in functional connectivity reflect perceived stimulus visibility.},
	language = {eng},
	journal = {NeuroImage},
	author = {Fatma Imamoglu and Jakob Heinzle and Adrian Imfeld and John-Dylan Haynes},
	month = nov,
	year = {2014},
	pmid = {25175537},
	keywords = {Adult, Female, Functional connectivity, Functional magnetic resonance imaging, Humans, Magnetic Resonance Imaging, Male, Neural correlates of consciousness, Occipital Lobe, Photic Stimulation, Temporal Lobe, Visual Cortex, Visual masking, Young Adult},
	pages = {688--694},
}

@article{imamoglu2012,
	title = {Changes in functional connectivity support conscious object recognition},
	volume = {63},
	issn = {1095-9572},
	doi = {10.1016/j.neuroimage.2012.07.056},
	abstract = {What are the brain mechanisms that mediate conscious object recognition? To investigate this question, it is essential to distinguish between brain processes that cause conscious recognition of a stimulus from other correlates of its sensory processing. Previous fMRI studies have identified large-scale brain activity ranging from striate to high-level sensory and prefrontal regions associated with conscious visual perception or recognition. However, the possible role of changes in connectivity during conscious perception between these regions has only rarely been studied. Here, we used fMRI and connectivity analyses, together with 120 custom-generated, two-tone, Mooney images to directly assess whether conscious recognition of an object is accompanied by a dynamical change in the functional coupling between extrastriate cortex and prefrontal areas. We compared recognizing an object versus not recognizing it in 19 naïve subjects using two different response modalities. We find that connectivity between the extrastriate cortex and the dorsolateral prefrontal cortex (DLPFC) increases when objects are consciously recognized. This interaction was independent of the response modality used to report conscious recognition. Furthermore, computing the difference in Granger causality between recognized and not recognized conditions reveals stronger feedforward connectivity than feedback connectivity when subjects recognized the objects. We suggest that frontal and visual brain regions are part of a functional network that supports conscious object recognition by changes in functional connectivity.},
	language = {eng},
	number = {4},
	journal = {NeuroImage},
	author = {Fatma Imamoglu and Thorsten Kahnt and Christof Koch and John-Dylan Haynes},
	month = dec,
	year = {2012},
	pmid = {22877578},
	keywords = {Adult, Causality, Consciousness, Female, Humans, Image Processing, Computer-Assisted, Magnetic Resonance Imaging, Male, Memory, Neural Pathways, Photic Stimulation, Prefrontal Cortex, Psychomotor Performance, Reaction Time, Recognition, Psychology, Visual Cortex, Visual Perception, Young Adult},
	pages = {1909--1917},
}

@misc{chen2024,
	title = {Bilingual language processing relies on shared semantic representations that are modulated by each language},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.06.24.600505v1},
	doi = {10.1101/2024.06.24.600505},
	abstract = {Billions of people throughout the world are bilingual and can understand semantic concepts in multiple languages. However, there is little agreement about how the brains of bilinguals represent semantic information from different languages. Some theories suggest that bilingual speakers’ brains contain separate representations for semantic information from different languages, while others suggest that different languages evoke the same semantic representations in the brain. To determine how the brains of bilinguals represent semantic information from different languages, we used functional magnetic resonance imaging (fMRI) to record brain responses while participants who are fluent in both English and Chinese read several hours of natural narratives in each language. We then used this data to specifically and comprehensively compare semantic representations between the two languages. We show that while semantic representations are largely shared between languages, these representations undergo fine-grained shifts between languages. These shifts systematically alter how different concept categories are represented in each language. Our results suggest that for bilinguals, semantic brain representations are shared across languages but modulated by each language. These results reconcile competing theories of bilingual language processing.},
	language = {en},
	urldate = {2025-10-02},
	publisher = {bioRxiv},
	author = {Catherine Chen and Xue L. Gong and Christine Tseng and Daniel L. Klein and Jack L. Gallant and Fatma Deniz},
	month = jun,
	year = {2024},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/UFEPVBQA/Chen et al. - 2024 - Bilingual language processing relies on shared semantic representations that are modulated by each l.pdf:application/pdf},
}

@article{wu2018,
	title = {The unified maximum a posteriori ({MAP}) framework for neuronal system identification},
	url = {http://arxiv.org/abs/1811.01043},
	doi = {10.48550/arXiv.1811.01043},
	abstract = {The functional relationship between an input and a sensory neuron's response can be described by the neuron's stimulus-response mapping function. A general approach for characterizing the stimulus-response mapping function is called system identification. Many different names have been used for the stimulus-response mapping function: kernel or transfer function, transducer, spatiotemporal receptive field. Many algorithms have been developed to estimate a neuron's mapping function from an ensemble of stimulus-response pairs. These include the spike-triggered average, normalized reverse correlation, linearized reverse correlation, ridge regression, local spectral reverse correlation, spike-triggered covariance, artificial neural networks, maximally informative dimensions, kernel regression, boosting, and models based on leaky integrate-and-fire neurons. Because many of these system identification algorithms were developed in other disciplines, they seem very different superficially and bear little relationship with each other. Each algorithm makes different assumptions about the neuron and how the data is generated. Without a unified framework it is difficult to select the most suitable algorithm for estimating the neuron's mapping function. In this review, we present a unified framework for describing these algorithms called maximum a posteriori estimation (MAP). In the MAP framework, the implicit assumptions built into any system identification algorithm are made explicit in three MAP constituents: model class, noise distributions, and priors. Understanding the interplay between these three MAP constituents will simplify the task of selecting the most appropriate algorithms for a given data set. The MAP framework can also facilitate the development of novel system identification algorithms by incorporating biophysically plausible assumptions and mechanisms into the MAP constituents.},
	urldate = {2025-10-02},
	publisher = {arXiv},
	journal = {arXiv},
	author = {Michael C.-K. Wu and Fatma Deniz and Ryan J. Prenger and Jack L. Gallant},
	month = nov,
	year = {2018},
	keywords = {Quantitative Biology - Neurons and Cognition},
	annote = {Comment: affiliations changed},
	file = {Preprint PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/C7VDJHZ4/Wu et al. - 2018 - The unified maximum a posteriori (MAP) framework for neuronal system identification.pdf:application/pdf;Snapshot:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/KZE6EUCD/1811.html:text/html},
}
