
@misc{negi2025a,
	title = {Optimizing {Language} {Model} {Embeddings} to {Voxel} {Activity} {Improves} {Brain} {Activity} {Predictions}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.09.18.676935v1},
	doi = {10.1101/2025.09.18.676935},
	abstract = {Recent studies have shown that contextual semantic embeddings from language models can accurately predict human brain activity during language processing. However, most studies use contextual embeddings with the same context length and model layer for all voxels, potentially overlooking meaningful variations across the brain. In this study, we investigate whether optimizing contextual embeddings for individual voxels improves their ability to predict brain activity during reading. We optimize embeddings for each voxel by selecting the best-predicting context length, model layer, or both. We perform this optimization with two different types of stimuli (isolated sentences and narratives), and quantify the performance gains of optimized embeddings over standard fixed embeddings. Our results show that voxel-specific optimization substantially improves the prediction accuracy of contextual semantic embeddings. These findings demonstrate that voxel-specific contextual tuning provides a more accurate and nuanced account of how the contextual semantic information is represented across the cortex.},
	language = {en},
	urldate = {2025-09-22},
	publisher = {bioRxiv},
	author = {Negi, Anuja and Tseng, Christine and Nunez-Elizalde, Anwar O. and Gong, Xue Lily and Deniz, Fatma},
	month = sep,
	year = {2025},
	note = {ISSN: 2692-8205
Pages: 2025.09.18.676935
Section: New Results},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/5SJ98ZPM/Negi et al. - 2025 - Optimizing Language Model Embeddings to Voxel Activity Improves Brain Activity Predictions.pdf:application/pdf},
	preview = {negi2025optimizing.jpg}
}

@misc{viscontidioleggiocastello2025,
	title = {Encoding models in functional magnetic resonance imaging: the {Voxelwise} {Encoding} {Model} framework},
	shorttitle = {Encoding models in functional magnetic resonance imaging},
	url = {https://sciety-labs.elifesciences.org/articles/by?article_doi=10.31234/osf.io/nt2jq_v2},
	abstract = {One of the major goals of cognitive neuroscience is understanding how the brain represents information about its own internal states and about the external world. This goal can be addressed by creating encoding models that reveal the information represented explicitly in measured brain activity. Here we describe the Voxelwise Encoding Model (VEM) framework for creating encoding models with functional magnetic resonance imaging (fMRI) data. The VEM framework provides several key advantages over traditional neuroimaging approaches. First, the VEM framework is applicable to most experimental designs, from classic factorial designs to complex naturalistic experiments such as movie watching or video games. This flexibility enables researchers to study brain function across multiple domains with the same analytical approach. Second, hypotheses about functional representations are defined and tested quantitatively by extracting feature spaces from experimental stimuli or tasks. These feature spaces quantify specific types of information potentially represented in brain activity and can range from simple human-derived labels to complex features generated by deep neural networks. If a feature space can be used to linearly predict brain activity, the brain is considered to explicitly represent features within that feature space. This prediction approach provides a systematic way to test hypotheses about functional representations. Third, the VEM framework implements robust data science methods to improve model estimation and minimize false positive results. Encoding models are estimated on a training set and validated on an independent test set. Testing in an independent dataset provides direct evidence that experimental findings generalize beyond the dataset used for model estimation. Finally, voxelwise encoding models can be created in each participant's native brain space without unnecessary information loss due to spatial averaging or template resampling required by conventional group analyses. This enables the VEM approach to reveal fine-grained functional organization in individual participants that might otherwise be ignored. In this article, we provide a comprehensive guide to the Voxelwise Encoding Model framework through all phases of research, from experimental design and data collection to the estimation and interpretation of voxelwise encoding models.},
	language = {en},
	urldate = {2025-09-22},
	publisher = {PsyArXiv},
	author = {Visconti di Oleggio Castello, M. and Deniz, Fatma and Dupré la Tour, Tom and Gallant, Jack L.},
	month = sep,
	year = {2025},
	file = {Snapshot:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/IKA9A2GT/by.html:text/html},
}

@misc{negi2025,
	title = {Brain-{Informed} {Fine}-{Tuning} for {Improved} {Multilingual} {Understanding} in {Language} {Models}},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2025.07.07.662360v1},
	doi = {10.1101/2025.07.07.662360},
	abstract = {Recent studies have demonstrated that fine-tuning language models with brain data can improve their semantic understanding, although these findings have so far been limited to English. Interestingly, similar to the shared multilingual embedding space of pretrained multilingual language models, human studies provide strong evidence for a shared semantic system in bilingual individuals. Here, we investigate whether fine-tuning language models with bilingual brain data changes model representations in a way that improves them across multiple languages. To test this, we fine-tune monolingual and multilingual language models using brain activity recorded while bilingual participants read stories in English and Chinese. We then evaluate how well these representations generalize to the bilingual participants’ first language, their second language, and several other languages that the participant is not fluent in. We assess the fine-tuned language models on brain encoding performance and downstream NLP tasks. Our results show that bilingual brain-informed fine-tuned language models outperform their vanilla (pretrained) counterparts in both brain encoding performance and most downstream NLP tasks across multiple languages. These findings suggest that brain-informed fine-tuning improves multilingual understanding in language models, offering a bridge between cognitive neuroscience and NLP research.},
	language = {en},
	urldate = {2025-09-22},
	publisher = {bioRxiv},
	author = {Negi, Anuja and Oota, Subba Reddy and Gupta, Manish and Deniz, Fatma},
	month = jul,
	year = {2025},
	note = {ISSN: 2692-8205
Pages: 2025.07.07.662360
Section: New Results},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/GL82YADH/Negi et al. - 2025 - Brain-Informed Fine-Tuning for Improved Multilingual Understanding in Language Models.pdf:application/pdf},
	preview = {negi2025brain.png}
}

@article{chen2024,
	title = {The cortical representation of language timescales is shared between reading and listening},
	volume = {7},
	copyright = {2024 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-024-05909-z},
	doi = {10.1038/s42003-024-05909-z},
	abstract = {Language comprehension involves integrating low-level sensory inputs into a hierarchy of increasingly high-level features. Prior work studied brain representations of different levels of the language hierarchy, but has not determined whether these brain representations are shared between written and spoken language. To address this issue, we analyze fMRI BOLD data that were recorded while participants read and listened to the same narratives in each modality. Levels of the language hierarchy are operationalized as timescales, where each timescale refers to a set of spectral components of a language stimulus. Voxelwise encoding models are used to determine where different timescales are represented across the cerebral cortex, for each modality separately. These models reveal that between the two modalities timescale representations are organized similarly across the cortical surface. Our results suggest that, after low-level sensory processing, language integration proceeds similarly regardless of stimulus modality.},
	language = {en},
	number = {1},
	urldate = {2025-09-22},
	journal = {Communications Biology},
	author = {Chen, Catherine and Dupré la Tour, Tom and Gallant, Jack L. and Klein, Daniel and Deniz, Fatma},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Language},
	pages = {284},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/5RFEBLPW/Chen et al. - 2024 - The cortical representation of language timescales is shared between reading and listening.pdf:application/pdf},
	preview = {chen2024-thumbnail.png}
}

@book{kitzes2017,
	title = {The {Practice} of {Reproducible} {Research} {Case} {Studies} and {Lessons} from the {Data}-{Intensive} {Sciences}},
	url = {https://www.ucpress.edu/books/the-practice-of-reproducible-research/paper},
	abstract = {Scholarship is a powerful tool for changing how people think, plan, and govern. By giving voice to bright minds and bold ideas, we seek to foster understanding and drive progressive change.},
	language = {en},
	urldate = {2025-09-22},
	editor = {Kitzes, Justin and Turek, Daniel and Deniz, Fatma},
	year = {2017},
	file = {Snapshot:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/BYTFYHV2/paper.html:text/html},
	preview = {The Practice of Reproducible Research-thumbnail.jpg}
}

@misc{elfaramawy2024,
	title = {On managing large collections of scientific workflows},
	url = {https://dl.gi.de/handle/20.500.12116/43762},
	publisher = {Gesellschaft für Informatik e.V.},
	author = {Elfaramawy, Nourhan and Deniz, Fatma and Grunske, Lars and Hilbrich, Marcus and Kehrer, Timo and Lamprecht, Anna-Lena and Mendling, Jan and Weidlich, Matthias},
	year = {2024},
	doi = {https://dl.gi.de/handle/20.500.12116/43762},
	note = {Publication title: Modellierung 2024 satellite events},
	preview = {elfaramawy et al 2024-thumbnail.png}
}

@article{gong2023,
	title = {Phonemic segmentation of narrative speech in human cerebral cortex},
	volume = {14},
	copyright = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-39872-w},
	doi = {10.1038/s41467-023-39872-w},
	abstract = {Speech processing requires extracting meaning from acoustic patterns using a set of intermediate representations based on a dynamic segmentation of the speech stream. Using whole brain mapping obtained in fMRI, we investigate the locus of cortical phonemic processing not only for single phonemes but also for short combinations made of diphones and triphones. We find that phonemic processing areas are much larger than previously described: they include not only the classical areas in the dorsal superior temporal gyrus but also a larger region in the lateral temporal cortex where diphone features are best represented. These identified phonemic regions overlap with the lexical retrieval region, but we show that short word retrieval is not sufficient to explain the observed responses to diphones. Behavioral studies have shown that phonemic processing and lexical retrieval are intertwined. Here, we also have identified candidate regions within the speech cortical network where this joint processing occurs.},
	language = {en},
	number = {1},
	urldate = {2025-09-22},
	journal = {Nature Communications},
	author = {Gong, Xue L. and Huth, Alexander G. and Deniz, Fatma and Johnson, Keith and Gallant, Jack L. and Theunissen, Frédéric E.},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cortex, Language, Neural encoding},
	pages = {4309},
	file = {Full Text PDF:/home/spathiphyllum/snap/zotero-snap/common/Zotero/storage/28FQ2WA7/Gong et al. - 2023 - Phonemic segmentation of narrative speech in human cerebral cortex.pdf:application/pdf},
	preview = {gong et al 2023-thumbnail.png}
}

@incollection{deniz2017,
	title = {{pyMooney}: {Generating} a {Database} of {Two}-{Tone}, {Mooney} {Images}},
	url = {https://www.practicereproducibleresearch.org/},
	booktitle = {The {Practice} of {Reproducible} {Research}: {Case} {Studies} in {Data} {Science}},
	author = {Deniz, F.},
	year = {2017},
	preview = {mooney_animation-thumbnail.gif}
}

@inproceedings{lamarre-etal-2022-attention,
    title = "Attention weights accurately predict language representations in the brain",
    author = "Lamarre, Mathis  and
      Chen, Catherine  and
      Deniz, Fatma",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.330/",
    doi = "10.18653/v1/2022.findings-emnlp.330",
    pages = "4513--4529",
    abstract = "In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the attention mechanism constructs contextual embeddings that carry information reflected in language-related brain representations. However, it is unclear whether the attention weights that are used to integrate information across words are themselves related to language representations in the brain. To address this question we analyzed functional magnetic resonance imaging (fMRI) recordings of participants reading English language narratives. We provided the narrative text as input to two LMs (BERT and GPT-2) and extracted their corresponding attention weights. We then used encoding models to determine how well attention weights can predict recorded brain responses. We find that attention weights accurately predict brain responses in much of the frontal and temporal cortices. Our results suggest that the attention mechanism itself carries information that is reflected in brain representations. Moreover, these results indicate cortical areas in which context integration may occur.",
    preview = {lamarre2022.png}
}


