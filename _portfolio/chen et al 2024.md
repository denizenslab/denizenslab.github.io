---
caption:
  title: Chen, et al. 2024
  subtitle: Nature Communications Biology
  thumbnail: assets/img/portfolio/chen2024-thumbnail.png

#what displays when the item is clicked:
title: "The cortical representation of language timescales is shared between reading and listening."
subtitle: "Chen C., Dupre la Tour T.,  Gallant J. L., Klein D., <b>Deniz F.</b>, <a href='https://www.nature.com/articles/s42003-024-05909-z'> The cortical representation of language timescales is shared between reading and listening. </a>  Nature Communications Biology 7, 284, 2024."
image: https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs42003-024-05909-z/MediaObjects/42003_2024_5909_Fig1_HTML.png?as=webp
#main image, can be a link or a file in assets/img/portfolio
alt: image alt text

---
##### Fig. 1: Timescale selectivity across the cortical surface.

<p style="text-align: justify">
Voxelwise modeling was used to determine the timescale selectivity of each voxel, for reading and listening separately (see Methods for details). a Timescale selectivity during listening (x axis) vs reading (y axis) for one representative participant (S1). Each point represents one voxel that was significantly predicted in both modalities. Points are colored according to the mean of the timescale selectivity during reading and listening. Blue denotes selectivity for short timescales, green denotes selectivity for intermediate timescales, and red denotes selectivity for long timescales. Timescale selectivity is significantly positively correlated between the two modalities (r = 0.41, P < 0.001). Timescale selectivity was also significantly positively correlated in the other eight participants (S2: r = 0.58, S3: r = 0.44, S4: 0.34, S5: 0.47, S6: 0.35, S7: 0.40, S8: 0.49, S9: 0.52, P < 0.001 for each participant). b Timescale selectivity during reading and listening on the flattened cortical surface of S1. Timescale selectivity is shown according to the color scale at the bottom (same color scale as in (a)). Voxels that were not significantly predicted are shown in gray (one-sided permutation test, P < 0.05, FDR-corrected; LH left hemisphere, RH right hemisphere, NS not significant, PFC prefrontal cortex, MPC medial parietal cortex, EVC early visual cortex, AC auditory cortex). For both modalities, temporal cortex contains a spatial gradient from intermediate to long-timescale selectivity along the superior to the inferior axis, the prefrontal cortex (PFC) contains a spatial gradient from intermediate to long-timescale selectivity along the posterior to the anterior axis, and precuneus is predominantly selective for long timescales. c Timescale selectivity in eight other participants. The format is the same as in (b). d Prediction performance for linguistic features (i.e., timescale-specific feature spaces) vs. low-level sensory features (i.e., spectrotemporal and motion energy feature spaces) for S1. Orange voxels were well-predicted by low-level sensory features. Blue voxels were well-predicted by linguistic features. White voxels were well-predicted by both sets of features. Low-level sensory features predict well in early visual cortex (EVC) during reading, and in early auditory cortex (AC) during listening. Linguistic features predict well in similar areas for reading and listening. After early sensory processing, cortical timescale representations are consistent between reading and listening across temporal, parietal, and prefrontal cortices.
</p>